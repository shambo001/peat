#labels Featured
<h1>DataPipeline</h1>
<img src=http://peat.googlecode.com/svn/wiki/images/PipeLine_logo.png width=300 align="right">
<wiki:toc max_depth="2" />
==Introduction==

DataPipeline is a python desktop and command line application that uses the fitting and plotting libraries from PEAT to automate the import of raw data in a variety of formats. It is also used for transforming the imported data through various stages of fitting to achieve a final measured parameter. This is useful for handling large amounts of data (e.g from csv files) in a consistent way without having to store everything in spreadsheets.

==Rationale==
Raw data from biology, chemistry and just about any laboratory experiments comes in a large variety of formats. Often the data format is proprietary but can often be converted to plain text or csv files for processing by the user. The problem is that the user then has to put the data into a spreadsheet and make sense of it. For one file this might be trivial, but for a lot of files that need processing it becomes laborious. Even when automated in a spreadsheet the workflow can be very confusing.

== Features ==

 * configuration file provides flexible import of raw data in text format
 * file names can be parsed and grouped by their labels (see examples)
 * possible to fit raw data and then chain these results to a further round of fitting
 * add your own non-linear fitting models using a special module
 * results output as csv files and plots if desired
 * works from command line as well as desktop application
 * programmers can add their own importers
 * apply pre-defined or custom filters/functions to the data set and chain them together to form a pipeline

==Workflow==

Shown below is the general workflow for the software. Boxes in dashed outline are optional steps.

<img src=http://peat.googlecode.com/svn/wiki/images/datapipeline_workflow.png width=500 align="right">

== Formats == 

The majority of text readable experimental data from, say, biochemical assays have formatting that can be categorised into pre-defined foramts. For example a table of data can either be presented with independent datasets arranged in '''rows or columns'''. Other variations are then subsets of these cases. (For our purposes 'dataset' refers to a single set of x-y points. The whole purpose of the exercise is to extract these x-y points.) The format is specified in the configuration file under the 'format' keyword, see Configuration. The typical formats you would generally expect to find are built into DataPipeline, that should cover a large percentage of possible cases that will arise. See DataPipelineFormats.

The chart below illustrates the concept graphically. You will be quite quickly able to see which one of these cases corresponds to your own data file. More complex combinations are possible of course, but this will be rarely the case for most users. For very specific and usual formats a custom importer class can be written to handle it and integrated into the application. Knowledge of python is required for this, see the API section below. 

http://peat.googlecode.com/svn/wiki/images/Dataformats_overview.png

===Grouped Data===
A special case that is sometimes encountered in biological assays are files with data grouped by some experimental condition for a range of samples. In this case the data is multidimensional.

== Configuration ==

A default configuration file is written by to the .pipeline folder in the users home directory and custom files can be adapted from this one and placed wherever the user wishes. A typical configuration file is as follows:

{{{
[base]
rowstart = 0
rowheader = 
yformat = 
format = databycolumn
decimalsymbol = .
workingdir = workingdir
colrepeat = 0
rowheaderstart = 0
rowrepeat = 0
preprocess = 
ignorecomments = 1
functionsconf = functions.conf
colheaderstart = 0
delimeter = ,
xformat = 
colheader = 
colend = 0
colstart = 0
rowend = 0

[files]
groupbyname = 0
replicates = 0
parsenamesindex = 0

[functions]
function1 = 

[models]
model1 = 

[variables]
variable1 = 

[excel]
numsheets = 1
sheet = 0

[custom]

[fitting]
xerror = 0
yerror = 0
modelsfile = 
iterations = 50

[plotting]
saveplots = 1
markersize = 25
normalise = 1
grayscale = 0
markers = -,x
linewidth = 1
fontsize = 9
marker = -
alpha = 0.7
font = sans-serif
showerrorbars = 0
legend = 0
dpi = 100
}}}

===Explanation of selected options===

Some options are self-explanatory, such as the plotting options. The functions section is explained in DataPipelineFunctions. The remainder are detailed below.

|| *option*|| *possible values* || *explanation* ||
||workingdir||any valid path||the folder where all results are placed||
||format||see formats diagram above||general structure of the data fall into predefined categories||
||decimalsymbol||. or ,||symbol used to indicate decimal point||
||delimiter||any symbol except a numerical value||separator between data||
||rowstart||any integer value||row where the data starts, including x labels||
||colstart||any integer value||column where the data starts, including y labels||
||rowend||any integer value||row where the data ends, optional||
||colend||any integer value||column where the data ends, optional||
||colrepeat||0 or any value >1||indicates that sets of data are grouped in evenly spaced columns||
||rowrepeat||0 or any value >1||indicates that sets of data are grouped in evenly spaced rows||
||ignorecomments||0 or 1||ignore lines starting with #||
||xformat||see 'specifying time formats' below||specify if x values in time formats||
||yformat||as above||specify if y values in time formats||
||xerror||any decimal||experimental error on all x values||
||yerror||any decimal||experimental error on all x values||
||iterations||any value greater than 1||number of rounds of fitting||
||modelsfile||any valid filename||file to load you models from||
||models||any valid model name||model to use for fitting see ModelDesign||

===Specifying time formats===
The xformat and yformat keywords allow you to specify if either dimension of your data are in units of time. These can then be converted to integer values of seconds for numerical analysis. For example if you x values increasing in amounts of 10 seconds in the folllowing format: 0:10, 0:20, 0:30. You enter *xformat = %M:%S* in the configuration file.

== Model Fitting ==

The 'pipeline' part of the application involves fitting imported data and grouping the fitted parameters into new sets of data which can in turn be fit to another model. Custom fitting models can be created using the ModelDesign module.

See [DataPipeline model fitting] for more technical information.

== Filtering and pre-precessing ==

See DataPipelineFunctions for information on specifying custom filters for the data. See also DataPipelineCaseStudy1.

== Desktop Application ==

http://peat.googlecode.com/svn/wiki/images/datapipeline_scr1.png

== API ==

DataPipeline is written in python and uses modules from the PEATDB project for plotting and fitting. For those interested in the programming interface and adding new importers, see DataPipelineAPI.

==Other links==

DataPipelineInstallation