==DataPipeline Case Study 3: Custom handling of multi-dimensional Kinetics data==

===Introduction===
The advent of high throughput protein and enzyme characterization technology has created the possibility of performing accurate biophysical measurements for a very large number of mutant proteins. The following example formed part of such a study: we studied the pH-dependence of kcat and KM for an enzyme using measurements of initial rates at 8 substrate concentrations at each of 10 pH values in triplicate. The ultimate goal was to track the change in kinetic parameters kcat/KM with pH in order to detect changes in the pKa value at the active site. We therefore had to fit three successive models to the data.

<img src=http://peat.googlecode.com/svn/wiki/images/datapipelinecasestudy3.png width=500 align="right">

===Format===
Since each initial rate measurement consisted of 90 absorbance/concentration measurements each enzyme clone resulted in 90x8x10x3 = 21,600 data points to be analyzed per mutant. The entire data set consisted of 100 mutant enzymes thus giving rise to approximately 2 million data points, which all must be analyzed consistently to give accurate and comparable values for the parameters of interest. The study was previously done using a set of spreadsheets with macros for plotting and fitting, resulting in speed issues and problems when any changes needed to be made by the actual users.

===Workflow===
The workflow was as follows:
 # Import the time vs. absorbance data at each substrate concentration for every variant, in triplicate.
 # Fit every raw dataset to a linear model to obtain a velocity at every concentration
 # Grouping these velocities versus their corresponding substrate concentrations gives us a dataset that we can fit to the Michaelis-Menten model and a yield KM value. Combined with a known enzyme concentration we can find kcat/KM
 # The final step was to group the kcat/KM results by pH and fit to the Henderson-Hasselbalch equation and extract a pKa value.

To handle the data a custom importer was written since it did not match any of our standard formats. (A more general version of this importer will be added as one of the standard formats). Each data file contained a set of columns corresponding to 12 variants with absorbance values for each time point on the y-axis. However the time points were also grouped for the 8 substrate concentrations, giving multi-dimensional data per file which needed to be extracted. The figure right shows how the data looked and also gives an overview of how the data was organized for fitting. 
We used the fit propagation functionality described elsewhere. Replicates were treated as independent and therefore fit separately. Note also that in our worked example we fit to KM values since finding the kcat requires an intermediate calculation step which we performed with a small amount of extra custom code. We are currently working on a more flexible way to specify automatic intermediate processing steps between successive iterations of fitting, in addition to the pre-processing step mentioned already.

===Configuration===

{{{
[base]
format = kineticsdata
rowstart = 3
colstart = 0
rowend = 0
colend = 12
rowheader = 3.2,1.6,0.8,0.4,0.2,0.1,0.05,0.025
colheader = wt 5,wt 3,wt 2,68 5,68 3,68 2,138 5,138 3,138 2,248 5,248 3,248 2
rowrepeat = 9
delimeter = tab
decimalsymbol = ,
xformat = %M:%S

[files]
groupbyname = 1
parsenamesindex = 2
replicates = 0

[fitting]
yerror = 0.01
iterations = 50

[models]
model1 = linear
model2 = Michaelis-Menten
model3 = sigmoid

[variables]
variable1 = a
variable2 = Km
variable3 = tm
}}}

===Sample data===
Sample data for a set of 12 variants over three replicates is available for download as a zip file [http://peat.googlecode.com/files/kinetics_data.zip here] and this case study can be tested in the application by loading one of the replicate folders and the configuration file included.